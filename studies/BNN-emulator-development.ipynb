{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning:Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import pints\n",
    "import pints.toy as toy\n",
    "\n",
    "import emupints\n",
    "import emupints.plot as emuplt\n",
    "import emupints.utils as emutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "from edward.models import Normal\n",
    "\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  2\n",
      "Parameter values:  [ 0.1 50. ]\n",
      "Example problem values:\n",
      " [3.19076239 2.09932542 4.50134443 7.21970643 3.74557539]\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = pints.toy.LogisticModel()\n",
    "\n",
    "n_parameters = model.n_parameters()\n",
    "n_outputs = model.n_outputs()\n",
    "real_parameters = model.suggested_parameters()\n",
    "\n",
    "values, times, noise = emutils.simulate(model,\n",
    "                                        parameters = real_parameters)\n",
    "\n",
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Create a log-likelihood function (adds an extra parameter!)\n",
    "real_log_likelihood = pints.KnownNoiseLogLikelihood(problem, noise)\n",
    "\n",
    "print(\"Number of parameters: \", n_parameters)\n",
    "print(\"Parameter values: \", real_parameters)\n",
    "print(\"Example problem values:\\n\", problem.values()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and normalizing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating distribution on bounds\n",
    "param_range = 0.5\n",
    "real_params_lower = (1 - param_range) * real_parameters\n",
    "real_params_upper = (1 + param_range) * real_parameters\n",
    "\n",
    "bounds = pints.Boundaries(lower = real_params_lower, upper = real_params_upper)\n",
    "log_prior = pints.UniformLogPrior(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training sample\n",
    "train_size = 500\n",
    "train_input = log_prior.sample(train_size)\n",
    "train_target = np.apply_along_axis(real_log_likelihood, 1, train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05050616, 25.23160115])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data between -0.5 and 0.5\n",
    "def normalize(x):\n",
    "    x_min = x.min(axis=0)\n",
    "    x_max = x.max(axis=0)\n",
    "    return (x - x_min) / (x_max - x_min) - 0.5\n",
    "\n",
    "def denormalize(x_norm, x_min, x_max):\n",
    "    return (x_norm + 0.5) * (x_max - x_min) + x_min\n",
    "    \n",
    "train_input_normalized = normalize(train_input)\n",
    "train_target_normalized = normalize(train_target)\n",
    "\n",
    "# define functions for future use\n",
    "train_input_min = train_input.min(axis=0)\n",
    "train_input_max = train_input.max(axis=0)\n",
    "\n",
    "train_target_min = train_target.min(axis=0)\n",
    "train_target_max = train_target.max(axis=0)\n",
    "\n",
    "def normalize_input(x):\n",
    "    return (x - train_input_min) / (train_input_max - train_input_min) - 0.5\n",
    "\n",
    "def denormalize_prediction(x_norm):\n",
    "    return denormalize(x_norm, train_target_min, train_target_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "bnn_shape = [n_parameters, 50, 10, 1]\n",
    "activations = tf.nn.relu\n",
    "\n",
    "# variational inference parameters\n",
    "n_iter=5000\n",
    "n_samples=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNNEmulator(emupints.Emulator):\n",
    "    \n",
    "    def __init__(self, log_likelihood, X, y, **kwargs):\n",
    "        super(BNNEmulator, self).__init__(log_likelihood, X, y, **kwargs)\n",
    "        \n",
    "        self._x = tf.placeholder(tf.float32, (None, self.n_parameters()))\n",
    "        self._sess = ed.get_session()\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        # convert to np array\n",
    "        if type(x) != np.ndarray:\n",
    "            x = np.asarray(x)\n",
    "        n_params = self.n_parameters()\n",
    "        x = x.reshape((x.size // n_params, n_params))\n",
    "\n",
    "        feed_dict = {self._x : x}        \n",
    "        # samples weights and biases from approximated posterior\n",
    "        qweights_sample = [\n",
    "            qW.sample() for qW in self._q_weights\n",
    "        ]\n",
    "        qbiases_sample = [\n",
    "            qb.sample() for qb in self._q_biases\n",
    "        ]\n",
    "        \n",
    "        nn = self._make_nn(qweights_sample,\n",
    "                            qbiases_sample, self.activations\n",
    "                           )\n",
    "        \n",
    "        return nn.eval(feed_dict=feed_dict)\n",
    "        \n",
    "\n",
    "    def set_parameters(self, shape, activations=tf.tanh):\n",
    "        # if only one activation function provided make list\n",
    "        if not hasattr(activations, '__len__'):\n",
    "            activations = [activations] * len(shape)\n",
    "            activations[-1] = tf.identity\n",
    "        self.activations = activations\n",
    "            \n",
    "        self.nn_shape = shape\n",
    "\n",
    "        self._set_priors(shape)\n",
    "        self._set_q_priors(shape)\n",
    "        \n",
    "        self._z = Normal(\n",
    "            loc=self._make_nn(self._weights, self._biases, activations),\n",
    "            scale=0.1 * tf.ones(tf.shape(self._x)[0])\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    # performs variational inference\n",
    "    def fit(self, n_iter=1000, n_samples=5):\n",
    "        # feed training data to the session \n",
    "        #self._sess.run()\n",
    "        \n",
    "        # connect corresponding variables with variational variables\n",
    "        qp_dict = dict(\n",
    "                zip([*self._weights, *self._biases],\n",
    "                    [*self._q_weights, *self._q_biases])\n",
    "        )\n",
    "        # target data\n",
    "        data = {\n",
    "            self._z : self._y.reshape(-1),\n",
    "            self._x : self._X\n",
    "        }\n",
    "        \n",
    "        inference = ed.KLqp(qp_dict, data=data)\n",
    "        inference.run(n_iter=n_iter, n_samples=n_samples, logdir='log')\n",
    "        \n",
    "        \n",
    "    # Create variable with proper normal prior\n",
    "    def _set_priors(self, shape):\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        # currently distrubitued as N(0, I)\n",
    "        for i in range(len(shape)-1):\n",
    "            W_i = Normal(loc=tf.zeros([shape[i], shape[i+1]]), \n",
    "                         scale=tf.ones([shape[i], shape[i+1]]))\n",
    "            b_i = Normal(loc=tf.zeros(shape[i+1]), \n",
    "                         scale=tf.ones(shape[i+1]))\n",
    "\n",
    "            self._weights.append(W_i)\n",
    "            self._biases.append(b_i)\n",
    "    \n",
    "    # sets variational priors\n",
    "    def _set_q_priors(self, shape):\n",
    "        self._q_weights = []\n",
    "        self._q_biases = []\n",
    "        # use get_var to allow parameters to vary\n",
    "        # initialized randomly\n",
    "        # softplus ensures positive variance\n",
    "        for i in range(len(shape)-1):\n",
    "            # creates/get variable for\n",
    "            qW_i_loc = tf.get_variable(f\"qW_{i}/loc\", [shape[i], shape[i+1]])\n",
    "            qW_i_scale = tf.get_variable(f\"qW_{i}/scale\", [shape[i], shape[i+1]])\n",
    "            qW_i = Normal(loc=qW_i_loc,\n",
    "                          scale=tf.nn.softplus(qW_i_scale))\n",
    "\n",
    "            qb_i_loc = tf.get_variable(f\"qb_{i}/loc\", [shape[i+1]])\n",
    "            qb_i_scale = tf.get_variable(f\"qb_{i}/scale\", [shape[i+1]])\n",
    "            qb_i = Normal(loc=qb_i_loc, \n",
    "                         scale=tf.nn.softplus(qb_i_scale))\n",
    "\n",
    "            self._q_weights.append(qW_i)\n",
    "            self._q_biases.append(qb_i)\n",
    "\n",
    "    def _make_nn(self, weights, biases, activations):\n",
    "        W_0, b_0, a_0 = weights[0], biases[0], activations[0]\n",
    "        W_last, b_last, a_last = weights[-1], biases[-1], activations[-1]\n",
    "\n",
    "        h = a_0(tf.matmul(self._x, W_0) + b_0)\n",
    "        for W_i, b_i, a_i in zip(weights[1:-1], biases[1:-1], activations[1:-1]):\n",
    "            h = a_i(tf.matmul(h, W_i) + b_i)\n",
    "        # usually a_last is identity\n",
    "        h = a_last(tf.matmul(h, W_last) + b_last)\n",
    "\n",
    "        return tf.reshape(h, [-1])\n",
    "    \n",
    "    def __str__():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create emulator for this instance\n",
    "emu_nn = BNNEmulator(real_log_likelihood, train_input_normalized, train_target_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify shape of NN\n",
    "emu_nn.set_parameters(bnn_shape, activations=activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /anaconda3/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning:Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 16s | Loss: 522.465\n"
     ]
    }
   ],
   "source": [
    "emu_nn.fit(n_iter=n_iter, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emu_nn_normalized = lambda x: denormalize_prediction(emu_nn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_error(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419.80252524211204\n"
     ]
    }
   ],
   "source": [
    "# look at mean error\n",
    "test_size = 200\n",
    "test_input = log_prior.sample(test_size)\n",
    "test_input_norm = normalize(test_input)\n",
    "\n",
    "test_true = np.apply_along_axis(real_log_likelihood, 1, test_input)\n",
    "test_pred = emu_nn_normalized(test_input_norm)\n",
    "\n",
    "print(mean_error(test_pred, test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
